{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9gVhkqmO6Fw"
      },
      "outputs": [],
      "source": [
        "# !pip install fitz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "# !pip install datasets\n",
        "# !pip install peft\n",
        "# !pip install accelerate\n",
        "# !pip install bitsandbytes\n",
        "# !pip install\n",
        "# !pip install torch"
      ],
      "metadata": {
        "id": "GWYEoXsXO8Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "import fitz  # Import the fitz module from PyMuPDF\n",
        "\n",
        "#Extract text from the given PDF file\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "  pdf_path =\"/content/Nlp and llm content.pdf\"\n",
        "  doc = fitz.open(pdf_path) # Now fitz is defined and can be used.\n",
        "  text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
        "  return text"
      ],
      "metadata": {
        "id": "JYtLihyLPQqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this import statement at the beginning of your script (ipython-input-22-d8fb31f8fa31)\n",
        "from datasets import Dataset\n",
        "\n",
        "def create_dataset_from_text(pdf_path):\n",
        "    raw_text = extract_text_from_pdf(pdf_path)\n",
        "    text_chunks = raw_text.split(\"\\n\\n\")  # Split text into manageable chunks\n",
        "\n",
        "    instructions, responses = [], []\n",
        "    for i in range(0, len(text_chunks) - 1, 2):  # Assume alternating Q/A format\n",
        "        instructions.append(text_chunks[i].strip())\n",
        "        responses.append(text_chunks[i + 1].strip())\n",
        "\n",
        "    dataset = Dataset.from_dict({\"instruction\": instructions, \"output\": responses}) # Now Dataset is defined and can be used.\n",
        "    return dataset\n",
        "\n",
        "dataset_path = \"/content/Nlp and llm content.pdf\"\n",
        "dataset = create_dataset_from_text(dataset_path)"
      ],
      "metadata": {
        "id": "zDVC_kyfPTys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Load the Model and Tokenizer with CPU offloading\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM # Importing AutoTokenizer and AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "def load_model_and_tokenizer(model_name=\"unsloth/Phi-4\"):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        load_in_4bit=True,  # Enable quantization for efficiency\n",
        "        llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offloading if necessary\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer()"
      ],
      "metadata": {
        "id": "hLcL579HPXKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#✅ Fine-tune the model with LoRA\n",
        "from peft import LoraConfig, get_peft_model # Import LoraConfig and get_peft_model\n",
        "\n",
        "def fine_tune_model(model, dataset, lora_rank=8):\n",
        "    lora_config = LoraConfig(\n",
        "        r=lora_rank,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # Enable Gradient Checkpointing\n",
        "    model.enable_input_require_grads()\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    return model\n",
        "\n",
        "model = fine_tune_model(model, dataset)"
      ],
      "metadata": {
        "id": "kQHRxPn2PX3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Generate response using the fine-tuned model\n",
        "def generate_response(prompt, model, tokenizer, max_length=256):\n",
        "    input_ids = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True\n",
        "    ).input_ids.to(model.device)\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=0.9,\n",
        "        top_k=40,\n",
        "        top_p=0.85,\n",
        "        do_sample=True,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "XE8W_C-WPbzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Test the fine-tuned model\n",
        "import datetime\n",
        "import time\n",
        "\n",
        "test_prompt = \"What is NLP and how is it categorized\"\n",
        "\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "response = generate_response(test_prompt, model, tokenizer)\n",
        "\n",
        "# Record end time and calculate execution time\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Print current time, execution time, and response\n",
        "print(f\"Current Time: {datetime.datetime.now()}\")\n",
        "print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
        "print(f\"Response: {response}\")"
      ],
      "metadata": {
        "id": "vv_5PB60Pcf8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}